{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'implementation of the Variational Recurrent\\nNeural Network (VRNN) from https://arxiv.org/abs/1506.02216\\nusing unimodal isotropic gaussian distributions for \\ninference, prior, and generating models.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt \n",
    "\"\"\"implementation of the Variational Recurrent\n",
    "Neural Network (VRNN) from https://arxiv.org/abs/1506.02216\n",
    "using unimodal isotropic gaussian distributions for \n",
    "inference, prior, and generating models.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from ta import *\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import math\n",
    "from numpy import reshape\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"S&P500_train.csv\") #Data from 1st Jan 2009 to 31 dec 2017\n",
    "test_data = pd.read_csv(\"S&P500_test.csv\") #Data from 1st Jan 2018 to 31 Dec 2018\n",
    "\n",
    "train_val = train_data['Volume']\n",
    "test_val = test_data['Volume']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "TIME_STEP = 29  # rnn time step / image height  \n",
    "train_val = array(train_val)\n",
    "train_val = train_val.reshape(train_val.shape[0], 1)\n",
    "train_val = scaler.fit_transform(train_val)\n",
    "train_x = series_to_supervised(train_val, n_in=TIME_STEP-1, n_out=-1)\n",
    "train_x = train_x.values\n",
    "train_y = series_to_supervised(train_val, n_in=TIME_STEP-2, n_out=1)\n",
    "train_y = train_y.iloc[1:]\n",
    "train_y = train_y.values\n",
    "\n",
    "\n",
    "test_val = array(test_val)\n",
    "test_val = test_val.reshape(test_val.shape[0], 1)\n",
    "test_val = scaler.fit_transform(test_val)\n",
    "test_x = series_to_supervised(test_val, n_in=TIME_STEP-1, n_out=-1)\n",
    "test_x = test_x.values\n",
    "test_y = series_to_supervised(test_val, n_in=TIME_STEP-2, n_out=1)\n",
    "test_y = test_y.iloc[1:]\n",
    "test_y = test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = reshape(train_x, (train_x.shape[0], 1, TIME_STEP-1, 1)) #n, lookback, predictors, N,T,P\n",
    "train_x = torch.from_numpy(train_x)\n",
    "train_y = reshape(train_y, (train_y.shape[0], 1, TIME_STEP-1, 1)) #n, lookback, predictors, N,T,P\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "test_x = reshape(test_x, (test_x.shape[0], 1, TIME_STEP-1, 1)) #n, lookback, predictors, N,T,P\n",
    "test_x = torch.from_numpy(test_x)\n",
    "test_y = reshape(test_y, (test_y.shape[0], 1, TIME_STEP-1, 1)) #n, lookback, predictors, N,T,P\n",
    "test_y = torch.from_numpy(test_y)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2236, 1, 28, 1]) torch.Size([2236, 1, 28, 1])\n",
      "torch.Size([224, 1, 28, 1]) torch.Size([224, 1, 28, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRNN(nn.Module):\n",
    "\tdef __init__(self, x_dim, h_dim, z_dim, n_layers, bias=False):\n",
    "\t\tsuper(SRNN, self).__init__()\n",
    "\t\tself.x_dim = x_dim\n",
    "\t\tself.h_dim = h_dim\n",
    "\t\tself.z_dim = z_dim\n",
    "\t\tself.n_layers = n_layers\n",
    "\n",
    "\n",
    "\t\t#encoder  x/u to z, input to latent variable, inference model\n",
    "\t\tself.enc = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, h_dim),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(h_dim, h_dim),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.enc_mean = nn.Linear(h_dim, z_dim)\n",
    "\t\tself.enc_std = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, z_dim),\n",
    "\t\t\tnn.Softplus())\n",
    "\n",
    "\t\t#prior transition of zt-1 to zt\n",
    "\t\tself.prior = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, h_dim),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.prior_mean = nn.Linear(h_dim, z_dim)\n",
    "\t\tself.prior_std = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, z_dim),\n",
    "\t\t\tnn.Softplus())\n",
    "\n",
    "\t\t#decoder from latent variable to output, from z to y\n",
    "\t\tself.dec = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim + h_dim, h_dim),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(h_dim, h_dim),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.dec_std = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, x_dim),\n",
    "\t\t\tnn.Softplus())\n",
    "\t\tself.dec_mean = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, x_dim),\n",
    "\t\t\tnn.Sigmoid())\n",
    "\n",
    "\t\t#recurrence backward RNN, another RNN fn here for SRNN\n",
    "\t\tself.rnn = nn.GRU(h_dim + h_dim, h_dim, n_layers, bias)    \n",
    "\t\tself.hidden_state_rnn = nn.GRU(h_dim, h_dim, n_layers, bias)          \n",
    "        \n",
    "\n",
    "\tdef forward(self, x): #flip h and pass through RNN, old x becomes y, new x becomes u, add response here later\n",
    "        # forward x and y\n",
    "        #generative and inference model\n",
    "\t\tall_enc_mean, all_enc_std = [], [] #inference\n",
    "\t\tall_dec_mean, all_dec_std = [], [] \n",
    "\t\tkld_loss = 0 #KL in ELBO\n",
    "\t\tnll_loss = 0 #-loglikihood in ELBO\n",
    "            \n",
    "\t\tz_t_sampled = []\n",
    "\t\tz_t = torch.zeros(self.n_layers, x.size(1), self.h_dim)[-1] \n",
    "           \n",
    "        # computing hidden state in list and x_t in list outside the loop\n",
    "\t\th = torch.zeros(self.n_layers, x.size(1), self.h_dim)   \n",
    "\t\th_list = []\n",
    "\t\tx_t_list = []        \n",
    "\t\tfor t in range(x.size(0)):\n",
    "\t\t\tx_t = x[t]\n",
    "\t\t\t_, h = self.hidden_state_rnn(torch.cat([x_t], 1).unsqueeze(0), h)            \n",
    "\t\t\tx_t_list.append(x_t)            \n",
    "\t\t\th_list.append(h[-1]) #append h not h[-1]\n",
    "               \n",
    "        #reversing hidden state list\n",
    "\t\treversed_h = h_list\n",
    "\t\treversed_h.reverse()\n",
    "        \n",
    "        #reversing x_t list\n",
    "\t\treversed_x_t = x_t_list\n",
    "\t\treversed_x_t.reverse()\n",
    "        \n",
    "        #concat reverse h with reverse x_t\n",
    "\t\tconcat_h_t_x_t_list = []\n",
    "\t\tfor t in range(x.size(0)):\n",
    "\t\t\tconcat_h_t_x_t = torch.cat([reversed_x_t[t], reversed_h[t]], 1).unsqueeze(0) \n",
    "\t\t\tconcat_h_t_x_t_list.append(concat_h_t_x_t)                            \n",
    "\n",
    "        #compute reverse a_t\n",
    "\t\ta_t = torch.zeros(self.n_layers, x.size(1), self.h_dim)\n",
    "\t\treversed_a_t_list = []\n",
    "\t\tfor t in range(x.size(0)):        \n",
    "\t\t\t_, a_t = self.rnn(concat_h_t_x_t_list[t], a_t) #RNN new         \n",
    "\t\t\treversed_a_t_list.append(a_t[-1])            \n",
    "\t\treversed_a_t_list.reverse()    \n",
    "\n",
    "\n",
    "\t\tfor t in range(x.size(0)):\n",
    "\t\t\tx_t = x[t] \n",
    "            \n",
    "\t\t\t#encoder\n",
    "\t\t\tenc_t = self.enc(reversed_a_t_list[t])\n",
    "\t\t\tenc_mean_t = self.enc_mean(enc_t)\n",
    "\t\t\tenc_std_t = self.enc_std(enc_t)    \n",
    "    \n",
    "\t\t\t#sampling and reparameterization, sampling from infer network\n",
    "\t\t\tz_t = self._reparameterized_sample(enc_mean_t, enc_std_t) \n",
    "\t\t\tz_t_sampled.append(z_t)             \n",
    "            \n",
    "\t\t\t#prior #transition, \n",
    "\t\t\tprior_t = self.prior(h_list[t])\n",
    "# \t\t\tprior_t = self.prior(torch.cat( (h_list[t], z_t), axis=1))          \n",
    "\t\t\tprior_mean_t = self.prior_mean(prior_t)\n",
    "\t\t\tprior_std_t = self.prior_std(prior_t)\n",
    "            \n",
    "\t\t\t#decoder #emission (generativemodel) \n",
    "\t\t\tdec_t = self.dec(torch.cat([z_t, h_list[t]], 1))              \n",
    "\t\t\tdec_mean_t = self.dec_mean(dec_t)\n",
    "\t\t\tdec_std_t = self.dec_std(dec_t)\n",
    "\n",
    "\t\t\t#computing losses\n",
    "\t\t\tkld_loss += self._kld_gauss(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n",
    "\t\t\tnll_loss += self._nll_bernoulli(dec_mean_t, x[t]) #change x[t] to y[t]\n",
    "\n",
    "\t\t\tall_enc_std.append(enc_std_t)\n",
    "\t\t\tall_enc_mean.append(enc_mean_t)\n",
    "\t\t\tall_dec_mean.append(dec_mean_t)\n",
    "\t\t\tall_dec_std.append(dec_std_t)  \n",
    "            \n",
    "\t\treturn kld_loss,nll_loss,(all_enc_mean, all_enc_std),(all_dec_mean, all_dec_std)\n",
    "            \n",
    "\n",
    "\tdef forecasting(self,x,step):\n",
    "\t\tall_enc_mean, all_enc_std = [], []\n",
    "\t\tall_dec_mean, all_dec_std = [], []\n",
    "\t\tz_t_sampled = []  \n",
    "\t\tz_t = torch.zeros(self.n_layers, x.size(1), self.h_dim)[-1]  \n",
    "        \n",
    "    \n",
    "        # computing hidden state in list and x_t in list outside the loop\n",
    "\t\th = torch.zeros(self.n_layers, x.size(1), self.h_dim)   \n",
    "\t\th_list = []\n",
    "\t\tx_t_list = []        \n",
    "\t\tfor t in range(x.size(0)):\n",
    "\t\t\tx_t = x[t]            \n",
    "\t\t\t_, h = self.hidden_state_rnn(torch.cat([x_t], 1).unsqueeze(0), h)            \n",
    "\t\t\tx_t_list.append(x_t)            \n",
    "\t\t\th_list.append(h[-1])\n",
    "               \n",
    "        #reversing hidden state list\n",
    "\t\treversed_h = h_list\n",
    "\t\treversed_h.reverse()\n",
    "        \n",
    "        #reversing x_t list\n",
    "\t\treversed_x_t = x_t_list\n",
    "\t\treversed_x_t.reverse()\n",
    "        \n",
    "        #concat reverse h with reverse x_t\n",
    "\t\tconcat_h_t_x_t_list = []\n",
    "\t\tfor t in range(x.size(0)):\n",
    "\t\t\tconcat_h_t_x_t = torch.cat([reversed_x_t[t], reversed_h[t]], 1).unsqueeze(0) \n",
    "\t\t\tconcat_h_t_x_t_list.append(concat_h_t_x_t)\n",
    "\n",
    "        #compute reverse a_t\n",
    "\t\ta_t = torch.zeros(self.n_layers, x.size(1), self.h_dim)\n",
    "\t\treversed_a_t_list = []\n",
    "\t\tfor t in range(x.size(0)):        \n",
    "\t\t\t_, a_t = self.rnn(concat_h_t_x_t_list[t], a_t) #RNN new         \n",
    "\t\t\treversed_a_t_list.append(a_t[-1])             \n",
    "\t\treversed_a_t_list.reverse()    \n",
    "\n",
    "\n",
    "\t\tfor t in range(x.size(0)):      \n",
    "\t\t\tx_t = x[t] \n",
    "\n",
    "\t\t\t#encoder   \n",
    "\t\t\tenc_t = self.enc(reversed_a_t_list[t])\n",
    "\t\t\tenc_mean_t = self.enc_mean(enc_t)\n",
    "\t\t\tenc_std_t = self.enc_std(enc_t)\n",
    "            \n",
    "\t\t\t#prior\n",
    "\t\t\tprior_t = self.prior(h_list[t])\n",
    "# \t\t\tprior_t = self.prior(torch.cat( (h_list[t], z_t), axis=1))    \n",
    "\t\t\tprior_mean_t = self.prior_mean(prior_t)\n",
    "\t\t\tprior_std_t = self.prior_std(prior_t)\n",
    "        \n",
    "\t\t\t#sampling and reparameterization\n",
    "\t\t\tz_t = self._reparameterized_sample(enc_mean_t, enc_std_t)\n",
    "\t\t\tz_t_sampled.append(z_t)\n",
    "            \n",
    "\t\t\t#decoder\n",
    "\t\t\tdec_t = self.dec(torch.cat([z_t, h_list[t]], 1))            \n",
    "\t\t\tdec_mean_t = self.dec_mean(dec_t)\n",
    "\t\t\tdec_std_t = self.dec_std(dec_t)\n",
    "            \n",
    "\t\t\tall_enc_std.append(enc_std_t)\n",
    "\t\t\tall_enc_mean.append(enc_mean_t)\n",
    "\t\t\tall_dec_mean.append(dec_mean_t)\n",
    "\t\t\tall_dec_std.append(dec_std_t)  \n",
    "    \n",
    "    \n",
    "\t\tx_predict=[]\n",
    "\t\tfor i in range(step):\n",
    "\t\t\t#prior \n",
    "\t\t\tprior_t = self.prior(h_list[t])\n",
    "# \t\t\tprior_t = self.prior(torch.cat( (h_list[i], z_t), axis=1))   \n",
    "\t\t\tprior_mean_t = self.prior_mean(prior_t)\n",
    "\t\t\tprior_std_t = self.prior_std(prior_t)\n",
    "            \n",
    "\t\t\t#sampling and reparameterization\n",
    "\t\t\tz_t = self._reparameterized_sample(enc_mean_t, enc_std_t)\n",
    "\t\t\tz_t_sampled.append(z_t)\n",
    "            \n",
    "\t\t\t#decoder\n",
    "\t\t\tdec_t = self.dec(torch.cat([z_t, h_list[i]], 1))                                                               \n",
    "\t\t\tdec_mean_t = self.dec_mean(dec_t)\n",
    "\t\t\tdec_std_t = self.dec_std(dec_t)\n",
    "\n",
    "\t\t\tx_t = dec_mean_t  \n",
    "\t\t\tx_predict.append(dec_mean_t)           \n",
    "                        \n",
    "\t\treturn x_predcict,z_t_sampled\n",
    "            \n",
    "\n",
    "\tdef reset_parameters(self, stdv=1e-1):\n",
    "\t\tfor weight in self.parameters():\n",
    "\t\t\tweight.normal_(0, stdv)\n",
    "\n",
    "\n",
    "\tdef _init_weights(self, stdv):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "\tdef _reparameterized_sample(self, mean, std):\n",
    "\t\t\"\"\"using std to sample\"\"\"\n",
    "\t\teps = torch.FloatTensor(std.size()).normal_()\n",
    "\t\treturn eps.mul(std).add_(mean)\n",
    "\n",
    "\n",
    "\tdef _kld_gauss(self, mean_1, std_1, mean_2, std_2):\n",
    "\t\t\"\"\"Using std to compute KLD\"\"\"\n",
    "\n",
    "\t\tkld_element =  (2 * torch.log(std_2) - 2 * torch.log(std_1) + \n",
    "\t\t\t(std_1.pow(2) + (mean_1 - mean_2).pow(2)) /\n",
    "\t\t\tstd_2.pow(2) - 1)\n",
    "\t\treturn\t0.5 * torch.sum(kld_element)\n",
    "\n",
    "\n",
    "\tdef _nll_bernoulli(self, theta, x):\n",
    "\t\treturn - torch.sum(x*torch.log(theta) + (1-x)*torch.log(1-theta))\n",
    "\n",
    "\n",
    "\tdef _nll_gauss(self, mean, std, x):\n",
    "\t\treturn  torch.sum(torch.log(std) + (x-mean).pow(2)/(2*std.pow(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"implementation of the Variational Recurrent\n",
    "Neural Network (VRNN) from https://arxiv.org/abs/1506.02216\n",
    "using unimodal isotropic gaussian distributions for \n",
    "inference, prior, and generating models.\"\"\"\n",
    "\n",
    "\n",
    "def train(epoch): # change model to take in x and y\n",
    "\ttrain_loss = 0\n",
    "\tfor batch_idx, (data, b_y) in enumerate(train_loader): #b_y not used\n",
    "# \tfor batch_idx, data in enumerate(train_loader): #b_y not used\n",
    "\n",
    "\t\t#transforming data\n",
    "\t\t#to remove eventually\n",
    "\t\tdata = data.squeeze().transpose(0, 1)\n",
    "# \t\tprint(data.size())        \n",
    "\t\tdata = (data - data.min().item()) / (data.max().item() - data.min().item())\n",
    "\t\t\n",
    "\t\t#forward + backward + optimize\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tkld_loss,nll_loss,_,_= model(data)\n",
    "\t\tloss = kld_loss + nll_loss\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t#grad norm clipping, only in pytorch version >= 1.10\n",
    "\t\tnn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "\t\t#printing\n",
    "\t\tif batch_idx % print_every == 0:\n",
    "\t\t\tprint('Train Epoch: {} [{}/{} ({:.0f}%)]\\t KLD Loss: {:.6f} \\t NLL Loss: {:.6f}'.format(\n",
    "\t\t\t\tepoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "\t\t\t\t100. * batch_idx / len(train_loader),\n",
    "\t\t\t\tkld_loss.item() / batch_size,\n",
    "\t\t\t\tnll_loss.item()/ batch_size))\n",
    "\n",
    "\t\ttrain_loss += loss.item()\n",
    "\n",
    "\tprint('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "\t\tepoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "\t\"\"\"uses test data to evaluate \n",
    "\tlikelihood of the model\"\"\"\n",
    "\t\n",
    "\tmean_kld_loss, mean_nll_loss = 0, 0\n",
    "\tfor i, (data, _) in enumerate(test_loader):                                            \n",
    "\t\t\n",
    "\t\tprint(data.size())\n",
    "\t\tdata = data.squeeze().transpose(0, 1)\n",
    "\t\tdata = (data - data.min().item()) / (data.max().item() - data.min().item())\n",
    "\n",
    "\t\tkld_loss, nll_loss,(ecoder_z_mean,ecoder_z_std), (decoder_z_mean,decoder_z_std) = model(data)\n",
    "\t\tmean_kld_loss += kld_loss.item()\n",
    "\t\tmean_nll_loss += nll_loss.item()\n",
    "\t\tx_predict,z_sampled = model.forecasting(data,3)\n",
    "\n",
    "\tmean_kld_loss /= len(test_loader.dataset)\n",
    "\tmean_nll_loss /= len(test_loader.dataset)\n",
    "\n",
    "\tprint('====> Test set loss: KLD Loss = {:.4f}, NLL Loss = {:.4f} '.format(\n",
    "\t\tmean_kld_loss, mean_nll_loss))\n",
    "\treturn x_predict,z_sampled,torch.stack(ecoder_z_mean),torch.stack(ecoder_z_std),torch.stack(decoder_z_mean),torch.stack(decoder_z_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\t KLD Loss: 25.262327 \t NLL Loss: 535.228027\n",
      "Train Epoch: 1 [2800/60000 (21%)]\t KLD Loss: 10.648190 \t NLL Loss: 230.556702\n",
      "Train Epoch: 1 [5600/60000 (43%)]\t KLD Loss: 11.496303 \t NLL Loss: 200.466660\n",
      "Train Epoch: 1 [8400/60000 (64%)]\t KLD Loss: 9.001026 \t NLL Loss: 190.098450\n",
      "Train Epoch: 1 [11200/60000 (85%)]\t KLD Loss: 9.453883 \t NLL Loss: 194.508057\n",
      "====> Epoch: 1 Average loss: 243.6082\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "====> Test set loss: KLD Loss = 9.2425, NLL Loss = 193.0071 \n",
      "Train Epoch: 2 [0/60000 (0%)]\t KLD Loss: 9.391180 \t NLL Loss: 191.799606\n",
      "Train Epoch: 2 [2800/60000 (21%)]\t KLD Loss: 8.638926 \t NLL Loss: 186.552383\n",
      "Train Epoch: 2 [5600/60000 (43%)]\t KLD Loss: 9.710012 \t NLL Loss: 191.479706\n",
      "Train Epoch: 2 [8400/60000 (64%)]\t KLD Loss: 9.548069 \t NLL Loss: 181.444199\n",
      "Train Epoch: 2 [11200/60000 (85%)]\t KLD Loss: 14.136881 \t NLL Loss: 173.893524\n",
      "====> Epoch: 2 Average loss: 195.6188\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "====> Test set loss: KLD Loss = 15.8983, NLL Loss = 168.5199 \n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "x_dim = 28\n",
    "h_dim = 28\n",
    "z_dim = 28\n",
    "\n",
    "n_layers =  1\n",
    "n_epochs = 2\n",
    "clip = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "seed = 128\n",
    "print_every = 100\n",
    "save_every = 2\n",
    "\n",
    "#manual seed\n",
    "torch.manual_seed(seed)\n",
    "plt.ion()\n",
    "\n",
    "#init model + optimizer + datasets\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=True, download=True,transform=transforms.ToTensor()),batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=False, transform=transforms.ToTensor()),batch_size=10000, shuffle=True)\n",
    "\n",
    "model = SRNN(x_dim, h_dim, z_dim, n_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\t\n",
    "\t#training + testing\n",
    "\ttrain(epoch)\n",
    "\tx_predict,z_sampled,ecoder_z_mean,ecoder_z_std,decoder_z_mean,decoder_z_std = test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "1\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "2\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "3\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "4\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "5\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "6\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "7\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "8\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "9\n",
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=False, \n",
    "                                                         transform=transforms.ToTensor()),batch_size=1000, \n",
    "                                          shuffle=True)\n",
    "for step, (x, y) in enumerate(test_loader): \n",
    "    print(step)\n",
    "    print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2237, 1, 28, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_val.shape\n",
    "train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "tensor([[0.6397],\n",
      "        [0.0298],\n",
      "        [0.8449],\n",
      "        [0.5720],\n",
      "        [0.6432],\n",
      "        [0.9019],\n",
      "        [0.2763],\n",
      "        [0.1102],\n",
      "        [0.9283],\n",
      "        [0.0893],\n",
      "        [0.0661],\n",
      "        [0.3583],\n",
      "        [0.6368],\n",
      "        [0.3135],\n",
      "        [0.1607],\n",
      "        [0.9967],\n",
      "        [0.7463],\n",
      "        [0.7010],\n",
      "        [0.7298],\n",
      "        [0.7175],\n",
      "        [0.3116],\n",
      "        [0.6796],\n",
      "        [0.9794],\n",
      "        [0.3878],\n",
      "        [0.9240],\n",
      "        [0.2409],\n",
      "        [0.3587],\n",
      "        [0.2676],\n",
      "        [0.7480],\n",
      "        [0.5804],\n",
      "        [0.3924],\n",
      "        [0.4838],\n",
      "        [0.5954],\n",
      "        [0.1991],\n",
      "        [0.4294],\n",
      "        [0.8762],\n",
      "        [0.6226],\n",
      "        [0.1010],\n",
      "        [0.9106],\n",
      "        [0.3699],\n",
      "        [0.6721],\n",
      "        [0.2570],\n",
      "        [0.6619],\n",
      "        [0.4265],\n",
      "        [0.9680],\n",
      "        [0.0919],\n",
      "        [0.5341],\n",
      "        [0.6802],\n",
      "        [0.2662],\n",
      "        [0.8802],\n",
      "        [0.3855],\n",
      "        [0.4835],\n",
      "        [0.3770],\n",
      "        [0.6056],\n",
      "        [0.6292],\n",
      "        [0.9661],\n",
      "        [0.3175],\n",
      "        [0.2093],\n",
      "        [0.8058],\n",
      "        [0.5286],\n",
      "        [0.1649],\n",
      "        [0.6769],\n",
      "        [0.1005],\n",
      "        [0.4732],\n",
      "        [0.1770],\n",
      "        [0.9182],\n",
      "        [0.9212],\n",
      "        [0.9819],\n",
      "        [0.5675],\n",
      "        [0.9713],\n",
      "        [0.7668],\n",
      "        [0.8930],\n",
      "        [0.8102],\n",
      "        [0.5944],\n",
      "        [0.2517],\n",
      "        [0.9419],\n",
      "        [0.7440],\n",
      "        [0.0075],\n",
      "        [0.0294],\n",
      "        [0.4674],\n",
      "        [0.5065],\n",
      "        [0.9046],\n",
      "        [0.9242],\n",
      "        [0.0652],\n",
      "        [0.6563],\n",
      "        [0.4575],\n",
      "        [0.4541],\n",
      "        [0.4814],\n",
      "        [0.8039],\n",
      "        [0.8935],\n",
      "        [0.7541],\n",
      "        [0.1363],\n",
      "        [0.6981],\n",
      "        [0.6361],\n",
      "        [0.0258],\n",
      "        [0.9530],\n",
      "        [0.7268],\n",
      "        [0.2875],\n",
      "        [0.5935],\n",
      "        [0.3048],\n",
      "        [0.9548],\n",
      "        [0.1094],\n",
      "        [0.8437],\n",
      "        [0.7788],\n",
      "        [0.4672],\n",
      "        [0.0352],\n",
      "        [0.2640],\n",
      "        [0.1789],\n",
      "        [0.2044],\n",
      "        [0.9702],\n",
      "        [0.7196],\n",
      "        [0.6477],\n",
      "        [0.2111],\n",
      "        [0.4588],\n",
      "        [0.7082],\n",
      "        [0.3362],\n",
      "        [0.8405],\n",
      "        [0.6638],\n",
      "        [0.7981],\n",
      "        [0.8903],\n",
      "        [0.5445],\n",
      "        [0.1340],\n",
      "        [0.6114],\n",
      "        [0.3438],\n",
      "        [0.8192],\n",
      "        [0.4420],\n",
      "        [0.5000],\n",
      "        [0.5893]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(1, 128, 1)\n",
    "t = t[-1, :, :]\n",
    "print(t.size())\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
