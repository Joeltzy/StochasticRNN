{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'implementation of the Variational Recurrent\\nNeural Network (VRNN) from https://arxiv.org/abs/1506.02216\\nusing unimodal isotropic gaussian distributions for \\ninference, prior, and generating models.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt \n",
    "\"\"\"implementation of the Variational Recurrent\n",
    "Neural Network (VRNN) from https://arxiv.org/abs/1506.02216\n",
    "using unimodal isotropic gaussian distributions for \n",
    "inference, prior, and generating models.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from ta import *\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import math\n",
    "from numpy import reshape\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ta/trend.py:170: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  dip[i] = 100 * (dip_mio[i]/trs[i])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ta/trend.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (dip_mio[i]/trs[i])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ta/trend.py:174: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  din[i] = 100 * (din_mio[i]/trs[i])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ta/trend.py:174: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (din_mio[i]/trs[i])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ta/trend.py:176: RuntimeWarning: invalid value encountered in subtract\n",
      "  dx = 100 * np.abs((dip - din) / (dip + din))\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"S&P500_train.csv\") #Data from 1st Jan 2009 to 31 dec 2017\n",
    "test_data = pd.read_csv(\"S&P500_test.csv\") #Data from 1st Jan 2018 to 31 Dec 2018\n",
    "train_val = add_all_ta_features(train_data, \"Open Price\", \"High Price\", \"Low Price\", \"Close Price\", \"Volume\", fillna=True)\n",
    "test_val = add_all_ta_features(test_data, \"Open Price\", \"High Price\", \"Low Price\", \"Close Price\", \"Volume\", fillna=True)\n",
    "\n",
    "train_val = train_val.drop(['Date'], axis=1)\n",
    "test_val = test_val.drop(['Date'], axis=1)\n",
    "train_val = train_val.iloc[:,0:28] # selecting the first 28 columns\n",
    "test_val = test_val.iloc[:,0:28] # selecting the first 28 columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "TIME_STEP = 29  # rnn time step / image height  \n",
    "\n",
    "train_val = scaler.fit_transform(train_val)\n",
    "train_x = series_to_supervised(train_val, n_in=TIME_STEP-1, n_out=-1)\n",
    "train_x = train_x.values\n",
    "train_x = train_x[:, : ]\n",
    "train_x = reshape(train_x, (train_x.shape[0], 1, TIME_STEP-1, 28))\n",
    "train_x = torch.from_numpy(train_x)\n",
    "\n",
    "train_y = series_to_supervised(train_val, n_in=TIME_STEP-2, n_out=1)\n",
    "train_y = train_y.iloc[1:]\n",
    "train_y = train_y.values\n",
    "train_y = train_y[:, : ]\n",
    "train_y = reshape(train_y, (train_y.shape[0], 1, TIME_STEP-1, 28))\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "\n",
    "test_val = scaler.fit_transform(test_val)\n",
    "test_x = series_to_supervised(test_val, n_in=TIME_STEP-1, n_out=-1)\n",
    "test_x = test_x.values\n",
    "test_x = test_x[:, : ]\n",
    "test_x = reshape(test_x, (test_x.shape[0], 1, TIME_STEP-1, 28))\n",
    "test_x = torch.from_numpy(test_x)\n",
    "\n",
    "test_y = series_to_supervised(test_val, n_in=TIME_STEP-2, n_out=1)\n",
    "test_y = test_y.iloc[1:]\n",
    "test_y = test_y.values\n",
    "test_y = test_y[:, : ]\n",
    "test_y = reshape(test_y, (test_y.shape[0], 1, TIME_STEP-1, 28))\n",
    "test_y = torch.from_numpy(test_y)\n",
    "\n",
    "train_x = train_x.double()\n",
    "train_y = train_y.double()\n",
    "test_x = test_x.double()\n",
    "test_y = test_y.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Preparation'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=28, shuffle=False)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=28, shuffle=False)\n",
    "\"\"\"Data Preparation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2236, 1, 28, 28]) torch.Size([2236, 1, 28, 28])\n",
      "torch.Size([224, 1, 28, 28]) torch.Size([224, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Shape of train and test set'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "\"\"\"Shape of train and test set\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRNN(nn.Module):\n",
    "\tdef __init__(self, x_dim, h_dim, z_dim, n_layers, bias=False):\n",
    "\t\tsuper(SRNN, self).__init__()\n",
    "\t\tself.x_dim = x_dim\n",
    "\t\tself.h_dim = h_dim\n",
    "\t\tself.z_dim = z_dim\n",
    "\t\tself.n_layers = n_layers\n",
    "\n",
    "\n",
    "\t\t#encoder  x/u to z, input to latent variable, inference model\n",
    "\t\tself.enc = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, h_dim),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(h_dim, h_dim),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.enc_mean = nn.Linear(h_dim, z_dim)\n",
    "\t\tself.enc_std = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, z_dim),\n",
    "\t\t\tnn.Softplus())\n",
    "\n",
    "\t\t#prior transition of zt-1 to zt\n",
    "\t\tself.prior = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, h_dim),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.prior_mean = nn.Linear(h_dim, z_dim)\n",
    "\t\tself.prior_std = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, z_dim),\n",
    "\t\t\tnn.Softplus())\n",
    "\n",
    "\t\t#decoder from latent variable to output, from z to y\n",
    "\t\tself.dec = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim + h_dim, h_dim),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(h_dim, h_dim),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.dec_std = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, x_dim),\n",
    "\t\t\tnn.Softplus())\n",
    "\t\tself.dec_mean = nn.Sequential(\n",
    "\t\t\tnn.Linear(h_dim, x_dim),\n",
    "\t\t\tnn.Sigmoid())\n",
    "\n",
    "\t\t#recurrence backward RNN, another RNN fn here for SRNN\n",
    "\t\tself.rnn = nn.GRU(h_dim + h_dim, h_dim, n_layers, bias)    \n",
    "\t\tself.hidden_state_rnn = nn.GRU(h_dim, h_dim, n_layers, bias)          \n",
    "        \n",
    "\n",
    "\tdef forward(self, x, y):\n",
    "        #generative and inference model\n",
    "\t\tall_enc_mean, all_enc_std = [], [] #inference\n",
    "\t\tall_dec_mean, all_dec_std = [], [] \n",
    "\t\tkld_loss = 0 #KL in ELBO\n",
    "\t\tnll_loss = 0 #-loglikihood in ELBO\n",
    "            \n",
    "\t\tz_t_sampled = []\n",
    "\t\tz_t = torch.zeros(self.n_layers, x.size(1), self.h_dim)[-1] \n",
    "           \n",
    "        # computing hidden state in list and x_t in list outside the loop\n",
    "\t\th = torch.zeros(self.n_layers, x.size(1), self.h_dim)   \n",
    "\t\th_list = []\n",
    "\t\tx_t_list = []  \n",
    "\n",
    "\t\tfor t in range(x.size(0)):\n",
    "\t\t\tx_t = x[t]       \n",
    "\t\t\t_, h = self.hidden_state_rnn(torch.cat([x_t], 1).unsqueeze(0), h)            \n",
    "\t\t\tx_t_list.append(x_t)            \n",
    "\t\t\th_list.append(h[-1]) #append h not h[-1]\n",
    "               \n",
    "        #reversing hidden state list\n",
    "\t\treversed_h = h_list\n",
    "\t\treversed_h.reverse()\n",
    "        \n",
    "        #reversing x_t list\n",
    "\t\treversed_x_t = x_t_list\n",
    "\t\treversed_x_t.reverse()  \n",
    "\n",
    "        #concat reverse h with reverse x_t\n",
    "\t\tconcat_h_t_x_t_list = []\n",
    " \n",
    "\t\tfor t in range(x.size(0)): #x.size(0) == y.size(0) == 28        \n",
    "\t\t\tconcat_h_t_x_t = torch.cat([reversed_x_t[t], reversed_h[t]], 1).unsqueeze(0) \n",
    "\t\t\tconcat_h_t_x_t_list.append(concat_h_t_x_t)\n",
    "              \n",
    "        #compute reverse a_t\n",
    "\t\ta_t = torch.zeros(self.n_layers, x.size(1), self.h_dim)\n",
    "\t\treversed_a_t_list = []\n",
    "\t\tfor t in range(x.size(0)):        \n",
    "\t\t\t_, a_t = self.rnn(concat_h_t_x_t_list[t], a_t) #RNN new      \n",
    "\t\t\treversed_a_t_list.append(a_t[-1])            \n",
    "\t\treversed_a_t_list.reverse() \n",
    "\n",
    "\t\tfor t in range(x.size(0)):\n",
    "\t\t\tx_t = x[t]\n",
    "            \n",
    "\t\t\t#encoder\n",
    "\t\t\tenc_t = self.enc(reversed_a_t_list[t])\n",
    "\t\t\tenc_mean_t = self.enc_mean(enc_t)\n",
    "\t\t\tenc_std_t = self.enc_std(enc_t)    \n",
    "    \n",
    "\t\t\t#sampling and reparameterization, sampling from infer network\n",
    "\t\t\tz_t = self._reparameterized_sample(enc_mean_t, enc_std_t) \n",
    "\t\t\tz_t_sampled.append(z_t)             \n",
    "            \n",
    "\t\t\t#prior #transition, \n",
    "\t\t\tprior_t = self.prior(h_list[t])\n",
    "# \t\t\tprior_t = self.prior(torch.cat( (h_list[t], z_t), axis=1))          \n",
    "\t\t\tprior_mean_t = self.prior_mean(prior_t)\n",
    "\t\t\tprior_std_t = self.prior_std(prior_t)\n",
    "            \n",
    "\t\t\t#decoder #emission (generativemodel) \n",
    "\t\t\tdec_t = self.dec(torch.cat([z_t, h_list[t]], 1))              \n",
    "\t\t\tdec_mean_t = self.dec_mean(dec_t)\n",
    "\t\t\tdec_std_t = self.dec_std(dec_t)\n",
    "\n",
    "\t\t\t#computing losses\n",
    "\t\t\tkld_loss += self._kld_gauss(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n",
    "\t\t\tnll_loss += self._nll_bernoulli(dec_mean_t, x[t]) #change x[t] to y[t]\n",
    "\n",
    "\t\t\tall_enc_std.append(enc_std_t)\n",
    "\t\t\tall_enc_mean.append(enc_mean_t)\n",
    "\t\t\tall_dec_mean.append(dec_mean_t)\n",
    "\t\t\tall_dec_std.append(dec_std_t)  \n",
    "            \n",
    "\t\treturn kld_loss,nll_loss,(all_enc_mean, all_enc_std),(all_dec_mean, all_dec_std)\n",
    "            \n",
    "\n",
    "\tdef forecasting(self,x,step):\n",
    "\t\tall_enc_mean, all_enc_std = [], []\n",
    "\t\tall_dec_mean, all_dec_std = [], []\n",
    "\t\tz_t_sampled = []  \n",
    "\t\tz_t = torch.zeros(self.n_layers, x.size(1), self.h_dim)[-1]  \n",
    "        \n",
    "    \n",
    "        # computing hidden state in list and x_t in list outside the loop\n",
    "\t\th = torch.zeros(self.n_layers, x.size(1), self.h_dim)   \n",
    "\t\th_list = []\n",
    "\t\tx_t_list = []        \n",
    "\t\tfor t in range(x.size(0)):\n",
    "\t\t\tx_t = x[t]            \n",
    "\t\t\t_, h = self.hidden_state_rnn(torch.cat([x_t], 1).unsqueeze(0), h)            \n",
    "\t\t\tx_t_list.append(x_t)            \n",
    "\t\t\th_list.append(h[-1])\n",
    "               \n",
    "        #reversing hidden state list\n",
    "\t\treversed_h = h_list\n",
    "\t\treversed_h.reverse()\n",
    "        \n",
    "        #reversing x_t list\n",
    "\t\treversed_x_t = x_t_list\n",
    "\t\treversed_x_t.reverse()\n",
    "        \n",
    "        #concat reverse h with reverse x_t\n",
    "\t\tconcat_h_t_x_t_list = []\n",
    "\t\tfor t in range(x.size(0)):\n",
    "\t\t\tconcat_h_t_x_t = torch.cat([reversed_x_t[t], reversed_h[t]], 1).unsqueeze(0) \n",
    "\t\t\tconcat_h_t_x_t_list.append(concat_h_t_x_t)\n",
    "\n",
    "        #compute reverse a_t\n",
    "\t\ta_t = torch.zeros(self.n_layers, x.size(1), self.h_dim)\n",
    "\t\treversed_a_t_list = []\n",
    "\t\tfor t in range(x.size(0)):        \n",
    "\t\t\t_, a_t = self.rnn(concat_h_t_x_t_list[t], a_t) #RNN new         \n",
    "\t\t\treversed_a_t_list.append(a_t[-1])             \n",
    "\t\treversed_a_t_list.reverse()    \n",
    "\n",
    "\n",
    "\t\tfor t in range(x.size(0)):      \n",
    "\t\t\tx_t = x[t] \n",
    "\n",
    "\t\t\t#encoder   \n",
    "\t\t\tenc_t = self.enc(reversed_a_t_list[t])\n",
    "\t\t\tenc_mean_t = self.enc_mean(enc_t)\n",
    "\t\t\tenc_std_t = self.enc_std(enc_t)\n",
    "            \n",
    "\t\t\t#prior\n",
    "\t\t\tprior_t = self.prior(h_list[t])\n",
    "# \t\t\tprior_t = self.prior(torch.cat( (h_list[t], z_t), axis=1))    \n",
    "\t\t\tprior_mean_t = self.prior_mean(prior_t)\n",
    "\t\t\tprior_std_t = self.prior_std(prior_t)\n",
    "        \n",
    "\t\t\t#sampling and reparameterization\n",
    "\t\t\tz_t = self._reparameterized_sample(enc_mean_t, enc_std_t)\n",
    "\t\t\tz_t_sampled.append(z_t)\n",
    "            \n",
    "\t\t\t#decoder\n",
    "\t\t\tdec_t = self.dec(torch.cat([z_t, h_list[t]], 1))            \n",
    "\t\t\tdec_mean_t = self.dec_mean(dec_t)\n",
    "\t\t\tdec_std_t = self.dec_std(dec_t)\n",
    "            \n",
    "\t\t\tall_enc_std.append(enc_std_t)\n",
    "\t\t\tall_enc_mean.append(enc_mean_t)\n",
    "\t\t\tall_dec_mean.append(dec_mean_t)\n",
    "\t\t\tall_dec_std.append(dec_std_t)  \n",
    "    \n",
    "    \n",
    "\t\tx_predict=[]\n",
    "\t\tfor i in range(step):\n",
    "\t\t\t#prior \n",
    "\t\t\tprior_t = self.prior(h_list[t])\n",
    "# \t\t\tprior_t = self.prior(torch.cat( (h_list[i], z_t), axis=1))   \n",
    "\t\t\tprior_mean_t = self.prior_mean(prior_t)\n",
    "\t\t\tprior_std_t = self.prior_std(prior_t)\n",
    "            \n",
    "\t\t\t#sampling and reparameterization\n",
    "\t\t\tz_t = self._reparameterized_sample(enc_mean_t, enc_std_t)\n",
    "\t\t\tz_t_sampled.append(z_t)\n",
    "            \n",
    "\t\t\t#decoder\n",
    "\t\t\tdec_t = self.dec(torch.cat([z_t, h_list[i]], 1))                                                               \n",
    "\t\t\tdec_mean_t = self.dec_mean(dec_t)\n",
    "\t\t\tdec_std_t = self.dec_std(dec_t)\n",
    "\n",
    "\t\t\tx_t = dec_mean_t  \n",
    "\t\t\tx_predict.append(dec_mean_t)           \n",
    "                        \n",
    "\t\treturn x_predict,z_t_sampled\n",
    "            \n",
    "\n",
    "\tdef reset_parameters(self, stdv=1e-1):\n",
    "\t\tfor weight in self.parameters():\n",
    "\t\t\tweight.normal_(0, stdv)\n",
    "\n",
    "\n",
    "\tdef _init_weights(self, stdv):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "\tdef _reparameterized_sample(self, mean, std):\n",
    "\t\t\"\"\"using std to sample\"\"\"\n",
    "\t\teps = torch.FloatTensor(std.size()).normal_()\n",
    "\t\treturn eps.mul(std).add_(mean)\n",
    "\n",
    "\n",
    "\tdef _kld_gauss(self, mean_1, std_1, mean_2, std_2):\n",
    "\t\t\"\"\"Using std to compute KLD\"\"\"\n",
    "\n",
    "\t\tkld_element =  (2 * torch.log(std_2) - 2 * torch.log(std_1) + \n",
    "\t\t\t(std_1.pow(2) + (mean_1 - mean_2).pow(2)) /\n",
    "\t\t\tstd_2.pow(2) - 1)\n",
    "\t\treturn\t0.5 * torch.sum(kld_element)\n",
    "\n",
    "\n",
    "\tdef _nll_bernoulli(self, theta, x):\n",
    "\t\treturn - torch.sum(x*torch.log(theta) + (1-x)*torch.log(1-theta))\n",
    "\n",
    "\n",
    "\tdef _nll_gauss(self, mean, std, x):\n",
    "\t\treturn  torch.sum(torch.log(std) + (x-mean).pow(2)/(2*std.pow(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"implementation of the Variational Recurrent\n",
    "Neural Network (VRNN) from https://arxiv.org/abs/1506.02216\n",
    "using unimodal isotropic gaussian distributions for \n",
    "inference, prior, and generating models.\"\"\"\n",
    "\n",
    "\n",
    "def train(epoch): # change model to take in x and y\n",
    "\ttrain_loss = 0\n",
    "\tfor batch_idx, (data, b_y) in enumerate(train_loader): #b_y not used\n",
    "      \n",
    "\t\tdata = data.float()        \n",
    "\t\t#transforming data\n",
    "\t\t#to remove eventually\n",
    "\t\tdata = data.squeeze().transpose(0, 1)       \n",
    "\t\tdata = (data - data.min().item()) / (data.max().item() - data.min().item())\n",
    "\n",
    "#########################################################################################    \n",
    "\t\tb_y = b_y.float()\n",
    "\t\tb_y = b_y.squeeze().transpose(0, 1)       \n",
    "\t\tb_y = (b_y - b_y.min().item()) / (b_y.max().item() - b_y.min().item())\n",
    "#########################################################################################  \n",
    "\n",
    "\t\t#forward + backward + optimize\n",
    "\t\toptimizer.zero_grad()          \n",
    "\t\tkld_loss,nll_loss,_,_= model(data, b_y)\n",
    "\t\tloss = kld_loss + nll_loss\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t#grad norm clipping, only in pytorch version >= 1.10\n",
    "\t\tnn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "\t\t#printing\n",
    "\t\tif batch_idx % print_every == 0:\n",
    "\t\t\tprint('Train Epoch: {} [{}/{} ({:.0f}%)]\\t KLD Loss: {:.6f} \\t NLL Loss: {:.6f}'.format(\n",
    "\t\t\t\tepoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "\t\t\t\t100. * batch_idx / len(train_loader),\n",
    "\t\t\t\tkld_loss.item() / batch_size,\n",
    "\t\t\t\tnll_loss.item()/ batch_size))\n",
    "\n",
    "\t\ttrain_loss += loss.item()\n",
    "\n",
    "\tprint('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "\t\tepoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "\t\"\"\"uses test data to evaluate \n",
    "\tlikelihood of the model\"\"\"\n",
    "\n",
    "\tmean_kld_loss, mean_nll_loss = 0, 0\n",
    "\tfor i, (data, b_y) in enumerate(test_loader):                                            \n",
    "\t\tdata = data.float()\n",
    "\t\tdata = data.squeeze().transpose(0, 1)\n",
    "\t\tdata = (data - data.min().item()) / (data.max().item() - data.min().item())\n",
    "\n",
    "#########################################################################################        \n",
    "\t\tb_y = b_y.float()\n",
    "\t\tb_y = b_y.squeeze().transpose(0, 1)       \n",
    "\t\tb_y = (b_y - b_y.min().item()) / (b_y.max().item() - b_y.min().item())\n",
    "#########################################################################################  \n",
    "\n",
    "\t\tkld_loss, nll_loss,(ecoder_z_mean,ecoder_z_std), (decoder_z_mean,decoder_z_std) = model(data, b_y)\n",
    "\t\tmean_kld_loss += kld_loss.item()\n",
    "\t\tmean_nll_loss += nll_loss.item()\n",
    "\t\tx_predict,z_sampled = model.forecasting(data,3)\n",
    "\n",
    "\tmean_kld_loss /= len(test_loader.dataset)\n",
    "\tmean_nll_loss /= len(test_loader.dataset)\n",
    "\n",
    "\tprint('====> Test set loss: KLD Loss = {:.4f}, NLL Loss = {:.4f} '.format(\n",
    "\t\tmean_kld_loss, mean_nll_loss))\n",
    "\treturn x_predict,z_sampled,torch.stack(ecoder_z_mean),torch.stack(ecoder_z_std),torch.stack(decoder_z_mean),torch.stack(decoder_z_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2236 (0%)]\t KLD Loss: 5.569289 \t NLL Loss: 117.673424\n",
      "====> Epoch: 1 Average loss: 490.5843\n",
      "====> Test set loss: KLD Loss = 0.5691, NLL Loss = 456.2270 \n",
      "Train Epoch: 2 [0/2236 (0%)]\t KLD Loss: 0.603447 \t NLL Loss: 118.457512\n",
      "====> Epoch: 2 Average loss: 453.7569\n",
      "====> Test set loss: KLD Loss = 0.6044, NLL Loss = 446.4348 \n",
      "Train Epoch: 3 [0/2236 (0%)]\t KLD Loss: 0.728471 \t NLL Loss: 114.245781\n",
      "====> Epoch: 3 Average loss: 448.8039\n",
      "====> Test set loss: KLD Loss = 0.3672, NLL Loss = 444.8991 \n",
      "Train Epoch: 4 [0/2236 (0%)]\t KLD Loss: 0.442594 \t NLL Loss: 111.049583\n",
      "====> Epoch: 4 Average loss: 443.5858\n",
      "====> Test set loss: KLD Loss = 0.2916, NLL Loss = 445.1673 \n",
      "Train Epoch: 5 [0/2236 (0%)]\t KLD Loss: 0.273043 \t NLL Loss: 108.194023\n",
      "====> Epoch: 5 Average loss: 432.1015\n",
      "====> Test set loss: KLD Loss = 0.3123, NLL Loss = 441.5865 \n",
      "Train Epoch: 6 [0/2236 (0%)]\t KLD Loss: 0.308859 \t NLL Loss: 108.673500\n",
      "====> Epoch: 6 Average loss: 423.8808\n",
      "====> Test set loss: KLD Loss = 0.2280, NLL Loss = 447.8725 \n",
      "Train Epoch: 7 [0/2236 (0%)]\t KLD Loss: 0.139298 \t NLL Loss: 93.350395\n",
      "====> Epoch: 7 Average loss: 403.9606\n",
      "====> Test set loss: KLD Loss = 0.1656, NLL Loss = 442.5596 \n",
      "Train Epoch: 8 [0/2236 (0%)]\t KLD Loss: 0.102540 \t NLL Loss: 77.899597\n",
      "====> Epoch: 8 Average loss: 394.3133\n",
      "====> Test set loss: KLD Loss = 0.1401, NLL Loss = 440.5721 \n",
      "Train Epoch: 9 [0/2236 (0%)]\t KLD Loss: 0.075824 \t NLL Loss: 70.409225\n",
      "====> Epoch: 9 Average loss: 390.9399\n",
      "====> Test set loss: KLD Loss = 0.1087, NLL Loss = 439.1071 \n",
      "Train Epoch: 10 [0/2236 (0%)]\t KLD Loss: 0.044530 \t NLL Loss: 68.279549\n",
      "====> Epoch: 10 Average loss: 389.8663\n",
      "====> Test set loss: KLD Loss = 0.0909, NLL Loss = 438.0312 \n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "x_dim = 28\n",
    "h_dim = 28\n",
    "z_dim = 28\n",
    "\n",
    "n_layers =  1\n",
    "n_epochs = 10\n",
    "clip = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "seed = 128\n",
    "print_every = 100\n",
    "save_every = 2\n",
    "\n",
    "#manual seed\n",
    "torch.manual_seed(seed)\n",
    "plt.ion()\n",
    "\n",
    "#init model + optimizer + datasets\n",
    "# train_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=True, download=True,transform=transforms.ToTensor()),batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=False, transform=transforms.ToTensor()),batch_size=10000, shuffle=True)\n",
    "\n",
    "model = SRNN(x_dim, h_dim, z_dim, n_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\t#training + testing\n",
    "\ttrain(epoch)\n",
    "\tx_predict,z_sampled,ecoder_z_mean,ecoder_z_std,decoder_z_mean,decoder_z_std = test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data, _) in enumerate(test_loader):\n",
    "    print(data.size(), _.size())\n",
    "    break\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=False, \n",
    "                                                         transform=transforms.ToTensor()),batch_size=1000, \n",
    "                                          shuffle=True)\n",
    "for i, (data, _) in enumerate(test_loader):\n",
    "    print(data.size(), _.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, b_y) in enumerate(train_loader):\n",
    "    print(data.size(), b_y.size())\n",
    "    break\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=True, download=True,transform=transforms.ToTensor()),batch_size=batch_size, shuffle=True)\n",
    "for batch_idx, (data, b_y) in enumerate(train_loader):\n",
    "    print(data.size(), b_y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "tensor([[0.6397],\n",
      "        [0.0298],\n",
      "        [0.8449],\n",
      "        [0.5720],\n",
      "        [0.6432],\n",
      "        [0.9019],\n",
      "        [0.2763],\n",
      "        [0.1102],\n",
      "        [0.9283],\n",
      "        [0.0893],\n",
      "        [0.0661],\n",
      "        [0.3583],\n",
      "        [0.6368],\n",
      "        [0.3135],\n",
      "        [0.1607],\n",
      "        [0.9967],\n",
      "        [0.7463],\n",
      "        [0.7010],\n",
      "        [0.7298],\n",
      "        [0.7175],\n",
      "        [0.3116],\n",
      "        [0.6796],\n",
      "        [0.9794],\n",
      "        [0.3878],\n",
      "        [0.9240],\n",
      "        [0.2409],\n",
      "        [0.3587],\n",
      "        [0.2676],\n",
      "        [0.7480],\n",
      "        [0.5804],\n",
      "        [0.3924],\n",
      "        [0.4838],\n",
      "        [0.5954],\n",
      "        [0.1991],\n",
      "        [0.4294],\n",
      "        [0.8762],\n",
      "        [0.6226],\n",
      "        [0.1010],\n",
      "        [0.9106],\n",
      "        [0.3699],\n",
      "        [0.6721],\n",
      "        [0.2570],\n",
      "        [0.6619],\n",
      "        [0.4265],\n",
      "        [0.9680],\n",
      "        [0.0919],\n",
      "        [0.5341],\n",
      "        [0.6802],\n",
      "        [0.2662],\n",
      "        [0.8802],\n",
      "        [0.3855],\n",
      "        [0.4835],\n",
      "        [0.3770],\n",
      "        [0.6056],\n",
      "        [0.6292],\n",
      "        [0.9661],\n",
      "        [0.3175],\n",
      "        [0.2093],\n",
      "        [0.8058],\n",
      "        [0.5286],\n",
      "        [0.1649],\n",
      "        [0.6769],\n",
      "        [0.1005],\n",
      "        [0.4732],\n",
      "        [0.1770],\n",
      "        [0.9182],\n",
      "        [0.9212],\n",
      "        [0.9819],\n",
      "        [0.5675],\n",
      "        [0.9713],\n",
      "        [0.7668],\n",
      "        [0.8930],\n",
      "        [0.8102],\n",
      "        [0.5944],\n",
      "        [0.2517],\n",
      "        [0.9419],\n",
      "        [0.7440],\n",
      "        [0.0075],\n",
      "        [0.0294],\n",
      "        [0.4674],\n",
      "        [0.5065],\n",
      "        [0.9046],\n",
      "        [0.9242],\n",
      "        [0.0652],\n",
      "        [0.6563],\n",
      "        [0.4575],\n",
      "        [0.4541],\n",
      "        [0.4814],\n",
      "        [0.8039],\n",
      "        [0.8935],\n",
      "        [0.7541],\n",
      "        [0.1363],\n",
      "        [0.6981],\n",
      "        [0.6361],\n",
      "        [0.0258],\n",
      "        [0.9530],\n",
      "        [0.7268],\n",
      "        [0.2875],\n",
      "        [0.5935],\n",
      "        [0.3048],\n",
      "        [0.9548],\n",
      "        [0.1094],\n",
      "        [0.8437],\n",
      "        [0.7788],\n",
      "        [0.4672],\n",
      "        [0.0352],\n",
      "        [0.2640],\n",
      "        [0.1789],\n",
      "        [0.2044],\n",
      "        [0.9702],\n",
      "        [0.7196],\n",
      "        [0.6477],\n",
      "        [0.2111],\n",
      "        [0.4588],\n",
      "        [0.7082],\n",
      "        [0.3362],\n",
      "        [0.8405],\n",
      "        [0.6638],\n",
      "        [0.7981],\n",
      "        [0.8903],\n",
      "        [0.5445],\n",
      "        [0.1340],\n",
      "        [0.6114],\n",
      "        [0.3438],\n",
      "        [0.8192],\n",
      "        [0.4420],\n",
      "        [0.5000],\n",
      "        [0.5893]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(1, 128, 1)\n",
    "t = t[-1, :, :]\n",
    "print(t.size())\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64, 1, 28, 28])\n",
      "torch.Size([1000, 1, 28, 28]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
